{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c68f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from urllib.parse import urlparse\n",
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "f9b50650",
   "metadata": {},
   "outputs": [],
   "source": [
    "url_list = ['https://www.cnbc.com/world/?region=world',\n",
    "            'https://finance.yahoo.com/',\n",
    "            'https://www.ft.com/',\n",
    "            'https://financialpost.com/']\n",
    "\n",
    "title_list = []\n",
    "h1_list = []\n",
    "h2_list = []\n",
    "h3_list = []\n",
    "article_list = []\n",
    "\n",
    "async def non_google_scrap():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "        for i in range(len(url_list)):\n",
    "            print(f'Scraping in {url_list[i]}')\n",
    "            page = await context.new_page()\n",
    "            await page.goto(url_list[i],timeout=30000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "            title = await page.title()\n",
    "            h1 = await page.locator(\"h1\").all_text_contents()\n",
    "            h2 = await page.locator(\"h2\").all_text_contents()\n",
    "            h3 = await page.locator(\"h3\").all_text_contents()\n",
    "            \n",
    "            if i >= 4:\n",
    "                article = await page.locator('div[aria-level=\"3\"][role=\"heading\"].n0jPhd.ynAwRc.MBeuO.nDgy9d, div[aria-level=\"3\"][role=\"heading\"]').all_text_contents()\n",
    "            else:  \n",
    "                article = await page.locator('a[title]').all_text_contents()\n",
    "\n",
    "            title_list.append(title)\n",
    "            h1_list.append(h1)\n",
    "            h2_list.append(h2)\n",
    "            h3_list.append(h3)\n",
    "\n",
    "             \n",
    "            article_list.append(article[:])\n",
    "            \n",
    "            print(f'Found {len(article[:])} articles in {url_list[i]}')\n",
    "            print(f'\\n')\n",
    "\n",
    "            await page.close()\n",
    "        await browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "e7fd1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scrap Google News\n",
    "url_list = ['https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en',\n",
    "            'https://news.google.com/rss/headlines/section/topic/BUSINESS?hl=en-US&gl=US&ceid=US:en',\n",
    "            'https://news.google.com/rss/headlines/section/topic/TECHNOLOGY?hl=en-US&gl=US&ceid=US:en']\n",
    "\n",
    "title_list = []\n",
    "h1_list = []\n",
    "h2_list = []\n",
    "h3_list = []\n",
    "article_list = []\n",
    "\n",
    "async def google_scrap():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        context = await browser.new_context(\n",
    "            user_agent=\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36\"\n",
    "        )\n",
    "        for i in range(len(url_list)):\n",
    "            print(f'Scraping in {url_list[i]}')\n",
    "            page = await context.new_page()\n",
    "            await page.goto(url_list[i],timeout=30000, wait_until=\"domcontentloaded\")\n",
    "\n",
    "            title = await page.title()\n",
    "            h1 = await page.locator(\"h1\").all_text_contents()\n",
    "            h2 = await page.locator(\"h2\").all_text_contents()\n",
    "            h3 = await page.locator(\"h3\").all_text_contents()\n",
    "            \n",
    "            article = await page.locator('item title').all_text_contents()\n",
    "\n",
    "            title_list.append(title)\n",
    "            h1_list.append(h1)\n",
    "            h2_list.append(h2)\n",
    "            h3_list.append(h3)\n",
    "\n",
    "             \n",
    "            article_list.append(article[:])\n",
    "            \n",
    "            print(f'Found {len(article[:])} articles in {url_list[i]}')\n",
    "            print(f'\\n')\n",
    "\n",
    "            await page.close()\n",
    "        await browser.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "a6fc94a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping in https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en\n",
      "Found 38 articles in https://news.google.com/rss?hl=en-US&gl=US&ceid=US:en\n",
      "\n",
      "\n",
      "Scraping in https://news.google.com/rss/headlines/section/topic/BUSINESS?hl=en-US&gl=US&ceid=US:en\n",
      "Found 27 articles in https://news.google.com/rss/headlines/section/topic/BUSINESS?hl=en-US&gl=US&ceid=US:en\n",
      "\n",
      "\n",
      "Scraping in https://news.google.com/rss/headlines/section/topic/TECHNOLOGY?hl=en-US&gl=US&ceid=US:en\n",
      "Found 57 articles in https://news.google.com/rss/headlines/section/topic/TECHNOLOGY?hl=en-US&gl=US&ceid=US:en\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Scrap Caller\n",
    "choice = input(\"Scrap with Google News or Other News? Enter G/N\")\n",
    "if choice.lower() == 'g':\n",
    "    await google_scrap()\n",
    "elif choice.lower == 'n':\n",
    "    await non_google_scrap()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "03d5e954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Handling\n",
    "flattened_list = []\n",
    "article_source = []\n",
    "\n",
    "web_index = 0\n",
    "\n",
    "for website in article_list:\n",
    "    domain = urlparse(url_list[web_index]).netloc.replace('www.','').replace('.com', '').title()\n",
    "    for article in website:\n",
    "        if article != \"\" and article != ' ':\n",
    "            flattened_list.append(article)\n",
    "            article_source.append(domain)\n",
    "    web_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "694b4ab6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Source</th>\n",
       "      <th>Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>No West Virginia National Guard troops deploye...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>U.S. halts all asylum decisions, pauses visas ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>Northwestern Agrees to Deal With Trump Adminis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>Three injured in Westfield Valley Fair shoppin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>Trump claims he will nullify executive orders ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>117</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>Museums are no longer afraid of ‘selling out’....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>Helldivers 2 devs correct “mistake” in Python ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>Here's the MacBook You Should Buy for Black Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>Use ChatGPT as Your iPhone's Action Button Ass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>News.Google</td>\n",
       "      <td>Tested: Windows 11’s ‘faster’ File Explorer (p...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>122 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          Source                                              Title\n",
       "0    News.Google  No West Virginia National Guard troops deploye...\n",
       "1    News.Google  U.S. halts all asylum decisions, pauses visas ...\n",
       "2    News.Google  Northwestern Agrees to Deal With Trump Adminis...\n",
       "3    News.Google  Three injured in Westfield Valley Fair shoppin...\n",
       "4    News.Google  Trump claims he will nullify executive orders ...\n",
       "..           ...                                                ...\n",
       "117  News.Google  Museums are no longer afraid of ‘selling out’....\n",
       "118  News.Google  Helldivers 2 devs correct “mistake” in Python ...\n",
       "119  News.Google  Here's the MacBook You Should Buy for Black Fr...\n",
       "120  News.Google  Use ChatGPT as Your iPhone's Action Button Ass...\n",
       "121  News.Google  Tested: Windows 11’s ‘faster’ File Explorer (p...\n",
       "\n",
       "[122 rows x 2 columns]"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DataFrame Creation\n",
    "df = pd.DataFrame(list(zip(article_source,flattened_list)),columns=['Source','Title'])\n",
    "#Extract only the news-like titles by length >= 30 (alphabetic)\n",
    "df_news = df[df['Title'].str.len() >= 30]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "63aac578",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "#Sentiment Analysis\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "Sentiment_Results = classifier(df_news['Title'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "1de92701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment Analysis Results\n",
      "Positive News Count is 45\n",
      "Negative News Count is 77\n",
      "Ratio of Positive/Negative is 0.584\n"
     ]
    }
   ],
   "source": [
    "positive_count = 0\n",
    "negative_count = 0\n",
    "\n",
    "for news in Sentiment_Results:\n",
    "    if news['label'] == 'POSITIVE':\n",
    "        positive_count += 1\n",
    "    elif news['label'] == 'NEGATIVE':\n",
    "        negative_count += 1\n",
    "\n",
    "print('Sentiment Analysis Results')\n",
    "print(f'Positive News Count is {positive_count}')\n",
    "print(f'Negative News Count is {negative_count}')\n",
    "print(f'Ratio of Positive/Negative is {round(positive_count/negative_count,3)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
